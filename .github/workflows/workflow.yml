on:
  schedule: 
    # runs once a day (for quick testing)
    - cron: "55 23 * * *"

jobs:
  # This workflow contains a single job called "traffic"
  traffic:
    # The type of runner that the job will run on
    runs-on: ubuntu-latest

    # Steps represent a sequence of tasks that will be executed as part of the job
    steps:
    # Checks-out your repository under $GITHUB_WORKSPACE, so your job can access it
    - uses: actions/checkout@v2
      with:
        ref: "main"

    - name: setup python
      uses: actions/setup-python@v5
      with:
        python-version: 3.12

    - name: Install dependencies
      run: python -m pip install pygithub

    - name: multiple repo traffic script
      run: python scan_multiple_github_repo_traffic.py repos_to_watch.txt traffic/all_views.csv traffic/all_views_unique.csv traffic/all_clones.csv traffic/all_clones_unique.csv traffic/referrers.json traffic/paths.json
      env:
        TRAFFIC_ACTION_TOKEN: ${{ secrets.TRAFFIC_ACTION_TOKEN }} 

    # Commits files to repository
    - name: Commit changes
      uses: EndBug/add-and-commit@v9
      with:
        author_name: Wandrille Duchemin
        message: "GitHub traffic"
        add: "./traffic/*"
        ref: "main"  # commits to branch "traffic"
    
    # Backup to S3 with date-based directory structure
    - name: Configure AWS credentials
      uses: aws-actions/configure-aws-credentials@v4
      with:
        aws-access-key-id: ${{ secrets.AWS_ACCESS_KEY_ID }}
        aws-secret-access-key: ${{ secrets.AWS_SECRET_ACCESS_KEY }}
        aws-region: ${{ secrets.AWS_REGION }}
    
    - name: Compress CSV files
      run: |
        # Create temporary directory for compressed files
        mkdir -p traffic_backup
        
        # Compress all CSV files (typically reduces size by 80-90%)
        for csv in ./traffic/*.csv; do
          if [ -f "$csv" ]; then
            gzip -c "$csv" > "traffic_backup/$(basename "$csv").gz"
            echo "Compressed: $(basename "$csv") -> $(basename "$csv").gz"
          fi
        done
        
        # Copy JSON files as-is (they don't compress as well)
        cp ./traffic/*.json traffic_backup/ 2>/dev/null || true
    
    - name: Backup traffic data to S3 with date
      run: |
        # Get current date in YYYY-MM-DD format
        BACKUP_DATE=$(date +%Y-%m-%d)
        
        # Upload compressed CSVs and JSON files to date-based S3 folder
        aws s3 cp ./traffic_backup s3://${{ secrets.AWS_S3_BUCKET }}/backups/${BACKUP_DATE}/ --recursive
        
        # Show what was uploaded
        echo "Backup completed to s3://${{ secrets.AWS_S3_BUCKET }}/backups/${BACKUP_DATE}/"
        du -sh traffic_backup
        
        # Clean up temporary directory
        rm -rf traffic_backup 
